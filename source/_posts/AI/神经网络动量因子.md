---
title: 神经网络动量因子
date: 2018-12-04 03:02:14
categories: AI
tags:
- DP
- NN
mathjax: true
---

反向传播算法中，学习率 $\eta$ 越小，每次迭代下降的步长越小，轨迹空间越平滑，学习速度越慢，提 $\eta$ 会加快学习速度，但网络权值的变化不稳定。为此，D.E. Rumlhart提出一种，既能加快学习速度，又能保持稳定的改进方法。这种方法是在修改规则中增加一个动量项：

$$
\Delta w(n)=-\eta\frac{\partial E(w)}{\partial w(n)} + a\Delta w(n - 1),\qquad n=1,2,3,...
$$
其中第一项为常规BP算法修正量，第二项为动量项，$a$ 为动量项系数，一般取 $(0,1)$。

直观上理解就是当前梯度方向与前一步梯度方向一致则增加权值更新，如果不一致则减小更新。类似于惯性。

> 参考：https://blog.csdn.net/bvl10101111/article/details/54973284